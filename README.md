# (Deep) Machine Learning Papers

A collection of (deep) machine learning papers.


## Learning algorithms

[The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://arxiv.org/pdf/1705.08292v1.pdf):
Adaptive Gradient Methods such as Adam converge faster and might even achieve better training error but
have worse test error than Stochastic Gradient Decent.


## Regularization

[Concrete Dropout](https://arxiv.org/pdf/1705.07832v1.pdf):
Automatic tuning of the dropout probability using gradient methods.


## Segmentation

[Dense Transformer Networks](https://arxiv.org/pdf/1705.08881v1.pdf):
Automatic learning of patch sizes and shapes in contrast to fixed, rectangular pixel centered patches
for segmentation. Achieves better segmentation.


## Saliency maps

[Real Time Image Saliency for Black Box Classifiers](https://arxiv.org/pdf/1705.07857v1.pdf):
Fast saliency detection method that can be applied to any differentiable image classifier.


## Unsupervised

[Look, Listen and Learn](https://arxiv.org/pdf/1705.08168v1.pdf):
Learning from unlabelled video and audio data.


## Reinforcement Learning

[Thinking Fast and Slow with Deep Learning and Tree Search](https://arxiv.org/pdf/1705.08439v1.pdf):
Decomposes the problem into separate planning and generalisation tasks and shows better performance than 
Policy Gradients.
